import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
from torchvision import transforms
from tqdm import tqdm
from typing import List, Union
from transformers import CLIPModel, CLIPProcessor  

class FashionCLIP:
    def __init__(self, model_name="patrickjohncyh/fashion-clip", device=None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = CLIPModel.from_pretrained(model_name).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model.eval()

    def encode_images(self, images: List[Union[Image.Image, torch.Tensor]], batch_size: int = 32) -> np.ndarray:
        """
        ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì„ë² ë”© ë°˜í™˜ (torch.Tensor ë˜ëŠ” PIL.Image.Image í—ˆìš©)
        HuggingFace Datasets ì œê±° ë²„ì „
                """

        def preprocess(img):
            if isinstance(img, torch.Tensor):
                # ì´ë¯¸ì§€: C x H x W â†’ H x W x C
                img = img.permute(1, 2, 0).cpu().numpy()

                # ê°’ ë²”ìœ„ [-1, 1] â†’ [0, 255]
                img = ((img + 1.0) * 127.5).clip(0, 255).astype(np.uint8)

                # numpy â†’ PIL ë³€í™˜
                img = Image.fromarray(img)

            return self.processor(images=img, return_tensors="pt")['pixel_values'].squeeze(0)


        preprocessed = [preprocess(img) for img in images]  # [C, H, W] í…ì„œ ë¦¬ìŠ¤íŠ¸
        image_tensor = torch.stack(preprocessed)  # [N, C, H, W]
        dataloader = torch.utils.data.DataLoader(image_tensor, batch_size=batch_size)

        image_embeddings = []
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="ğŸ”„ Encoding Images"):
                batch = batch.to(self.device)
                embeds = self.model.get_image_features(pixel_values=batch)
                embeds = F.normalize(embeds, dim=1)  # ì •ê·œí™”
                image_embeddings.append(embeds.cpu().numpy())

        return np.concatenate(image_embeddings, axis=0)